## qt_01_img_to_melody (이미지마다 다른 bgm  재생)

```python
import serial

import tensorflow as tf

# from tensorflow.keras.models import load_model
from keras.models import load_model
import cv2
import numpy as np

import threading, time


IMG_WIDTH = 224
IMG_HEIGHT = 224


lock = threading.Lock()

class Worker(threading.Thread):

    def __init__(self):
        threading.Thread.__init__(self) 
        self.daemon = True
 
    working = True
    def run(self):
        while self.working:
            if ser.readable():
                result = ser.readline()
                if not result:
                    time.sleep(0.1)
                    continue

                result = result.decode()
                result = result.replace('\r\n', '')
                # print(result, end='')
                # print(result)

                readMsg(result)               
                time.sleep(0.1)

        ser.close()

    # def stop(self):
    #     self.working = False
    #     # self.quit()
    #     # self.wait(5000)


playable = False

def prepare():
    np.set_printoptions(suppress=True)

    cv2.namedWindow("Img")
    cv2.setMouseCallback("Img", mouseEvent)

    global model, classNames, playable
    # model = load_model('./res/model/keras_model.h5')
    # classNames = open('./res/model/labels.txt', 'r').readlines()

    model = load_model('./res/model_ma_li_te/keras_model.h5')
    classNames = open('./res/model_ma_li_te/labels.txt', 'r').readlines()

    playable = False

def loop():

    global playable

    cap = cv2.VideoCapture(0)

    while True:

        ret, frame = cap.read()
        if not ret:
            if cv2.waitKey(24) == 27:
                break

            continue

        # 티처블머신 스펙 (224 x 224) 이미지 크기로 설정
        image = cv2.resize(frame, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_AREA)
        cv2.imshow("Img", image)

        # print('playable:: ', playable)

        if playable:
            image = np.asarray(image, dtype=np.float32).reshape(1, IMG_WIDTH, IMG_HEIGHT, 3)
            image = (image / 127.5)-1 # 노멀라이즈, 0.0 ~ 1.0

            prediction = model.predict(image)
            index = np.argmax(prediction) # 가장 높은 값의 인덱스를 반환
            # print('prediction index: ', index)

            className = classNames[index]
            confidenceScore = prediction[0][index]

            # [prediction]
            # [
            #  [0.4, 1.0, 0.8]
            # ]

            # print('prediction: ', prediction)

            # print('confidenceScore: ', confidenceScore)
            print("Class:", className[2:], end="")
            # '1.1122'[:-2] => '1.11'
            print("Confidence Score:", str(np.round(confidenceScore * 100))[:-2], "%")

            if confidenceScore >= 0.5:
                # 아두이노로 시리얼통신을 통해 명령 전달
                sendMsg(str(index+1))
                playable = False

        if cv2.waitKey(24) == 27:
            break


def sendMsg(msg:str):
    ser.write(msg.encode()) #.encode() -> bytes 

def readMsg(msg:str):

    global playable

    # 쓰레드락
    lock.acquire()
    
    playable = msg == 'e'
    
    # 쓰레드 릴리즈
    lock.release()


def mouseEvent(event, x, y, flags, param):

    global playable

    if event == cv2.EVENT_LBUTTONDOWN:
        print('마우스 버튼 눌림')
        playable = True

    if event == cv2.EVENT_LBUTTONUP:
        print('마우스 버튼 떨어짐')
        playable = False
    

    # if event == cv2.EVENT_FLAG_LBUTTON:    
    #     cv2.circle(param, (x, y), radius, (255, 0, 0), 2)
    #     cv2.imshow("draw", src)

PORT = '/dev/ttyUSB0'

if __name__ == '__main__':

    ser = serial.serial_for_url(PORT, baudrate=9600, timeout=1)

    worker = Worker()
    worker.start()

    prepare()
    loop()

```

## yolo_yt.py

```python
import torch
import cv2
from cap_from_youtube import cap_from_youtube

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

model = torch.hub.load('ultralytics/yolov5', "yolov5s")
model.to(device) #현재 기본은 cuda
classNames = model.names

url = "https://www.youtube.com/watch?v=h3glVQbTmks"

# cap = cap_from_youtube(url, '480p')
cap = cv2.VideoCapture(0)
fps = cap.get(cv2.CAP_PROP_FPS) # ex: 30fps, 60fps
mfps = int(1000/fps)  # ex: 16ms, 30ms
print('frameRate: ', fps)
print('mfps: ', mfps)

while cap.isOpened():
    ret, frame = cap.read()

    if ret:
        # 프레임을 읽어와 표시(합성된 결과)

        # 해당 프레임을 yolo를 활용해 파악
        predict = model(frame)

        labels = predict.xyxyn[0][:,-1].cpu().numpy()
        cord = predict.xyxyn[0][:,:-1].cpu().numpy()

        # 결과를 프레임과 합성
        fy, fx = frame.shape[:2]

        labelNum = len(labels)
        for i in range(labelNum):
            # row => 해당 행의 값들이 담겨있음
            row = cord[i]
            x = row[4]
            if x >= 0.2:
                # 0.5 * 1080 = 540 = x1
                x1 = int(row[0]*fx)
                y1 = int(row[1]*fy)
                x2 = int(row[2]*fx)
                y2 = int(row[3]*fy)
                
                # labelStr = f'{classNames[int(labels[i])]}, ({x1},{y1}), {str(row[4])[:4]}'
                labelStr = '{}, ({},{}), {}'.format(
                                classNames[int(labels[i])],
                                x1,y1,
                                str(row[4])[:4]
                            )
                
                cv2.rectangle(frame, (x1,y1,x2,y2), (0,255,0), 2)
                cv2.putText(frame, labelStr, 
                            (x1,y1), cv2.FONT_HERSHEY_SIMPLEX, 1, 
                            (0,0,255), 2)
                
        cv2.imshow('dst', frame)

    if cv2.waitKey(mfps) == 27:
        break

cap.release()
cv2.destroyAllWindows()
```

## yolo_webcam.py

```python
import torch
import cv2
import time

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

# cuda = torch.device(device)

# Model
model = torch.hub.load("ultralytics/yolov5", "yolov5s")
# model.cuda()
# model.to(device)
# model.to('cuda')
# model.to('cpu')

classes = model.names

cap = cv2.VideoCapture(0)

while cap.isOpened():

    ret, frame = cap.read()
    if not ret:
        if cv2.waitKey(1) == 27:
            break
        continue

    now = time.time()
    
    # TODO 모델을 활용해서 객체를 탐색하고 탐색된 결과를 합성
    
    # 모델을 통해 해당 프레임을 분석
    predict = model(frame) #해당 프레임의 이미지를 특정 모델을 통해 이미지속 사물을 파악

    labels = predict.xyxyn[0][:,-1].cpu().numpy()  #파악된 결과속 정보 중 라벨만 읽어옴 (narray로 구성)
    cord = predict.xyxyn[0][:,:-1].cpu().numpy()   #파악된 결과속 정보 중 위치만 읽어옴 (narray로 구성)

    # 읽어온 정보를 기반으로 합성
    frameY, frameX = frame.shape[:2]

    for i in range(len(labels)):
        row = cord[i]
        if row[4] >= 0.2:
            x1 = int(row[0]*frameX)
            y1 = int(row[1]*frameY)
            x2 = int(row[2]*frameX)
            y2 = int(row[3]*frameY)

            labelStr = f'{classes[int(labels[i])]}, ({x1},{x2}), {str(row[4])[:4]}'

            cv2.rectangle(frame, (x1,y1,x2,y2), (0,255,0), 2)
            cv2.putText(frame, labelStr, (x1,y1), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)

    cv2.imshow('origin', frame)

    if cv2.waitKey(1) == 27:
        break

    delta = time.time() - now
    print(delta)

```
